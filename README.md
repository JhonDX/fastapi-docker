# ğŸš€ Pipeline de ConsolidaÃ§Ã£o de Dados do Tesouro Direto  
### Data Engineering â€¢ DataOps â€¢ Docker â€¢ Airflow â€¢ PostgreSQL

---

## ğŸ“Œ Sobre o Projeto

Este projeto implementa um pipeline completo de Engenharia de Dados responsÃ¡vel por consolidar, transformar e estruturar dados pÃºblicos do **Tesouro Direto**, automatizando todo o fluxo de ingestÃ£o atÃ© a carga final em banco relacional.

### A soluÃ§Ã£o simula um cenÃ¡rio real de mercado onde mÃºltiplos arquivos CSV precisam ser:

- Consolidados  
- Padronizados  
- Modelados  
- Automatizados  
- Orquestrados  

Tudo isso em ambiente containerizado e reprodutÃ­vel.

---

## ğŸ¯ Problema Resolvido

Os dados pÃºblicos do Tesouro Direto sÃ£o disponibilizados em mÃºltiplos arquivos CSV e nÃ£o estÃ£o estruturados para anÃ¡lise direta.

### Este projeto resolve:

âœ” ConsolidaÃ§Ã£o de mÃºltiplas fontes  
âœ” PadronizaÃ§Ã£o de datas e valores monetÃ¡rios  
âœ” Modelagem relacional  
âœ” AutomatizaÃ§Ã£o do processo  
âœ” Reprodutibilidade com Docker  
âœ” OrquestraÃ§Ã£o com Apache Airflow  

---

## ğŸ— Arquitetura da SoluÃ§Ã£o

- CSV (dados brutos)
- â†“
- ConcatenaÃ§Ã£o
- â†“
- Pipeline ETL (Pandas)
- â†“
- PostgreSQL (Banco do Projeto)
- â†“
- OrquestraÃ§Ã£o via Airflow
- â†“
- Ambiente Containerizado


---

## ğŸ›  Tecnologias Utilizadas

- Python  
- Pandas  
- PostgreSQL  
- Apache Airflow  
- Docker & Docker Compose  
- SQLAlchemy  

---

## ğŸ—„ Estrutura de Bancos

O projeto utiliza:

### ğŸ”¹ Banco de Metadados do Airflow

ResponsÃ¡vel por armazenar:

- DAGs  
- Logs  
- HistÃ³rico de execuÃ§Ã£o  

---

## ğŸ”¹ Banco do Projeto

Banco onde o pipeline grava as tabelas tratadas:

- `titulo`  
- `movimentacao`  

### ğŸ“Œ **Importante:**  
O nome do banco do projeto deve ser definido nas variÃ¡veis de ambiente.  
No desenvolvimento deste projeto foi utilizado o banco:

tesouro

Certifique-se de que o banco esteja criado e configurado corretamente no ambiente Docker antes de executar a DAG.

## âš™ï¸ Como Executar o Projeto

### 1ï¸âƒ£ Clone o repositÃ³rio

- git clone https://github.com/JhonDX/fastapi-docker.git
- cd fastapi-docker

### 2ï¸âƒ£ Configure as variÃ¡veis de ambiente
### Defina corretamente:

- Credenciais do banco

- Nome do banco do projeto

- Caminho do dataset

- ConfiguraÃ§Ã£o do Airflow

### ğŸ“Œ O banco do projeto deve estar configurado (ex: tesouro).

### 3ï¸âƒ£ Suba os containers

- docker compose up -d

### 4ï¸âƒ£ Acesse o Airflow
- http://localhost:8080
- Ative a DAG e execute manualmente para rodar o pipeline.

### ğŸ”„ Fluxo do Pipeline

- ConcatenaÃ§Ã£o dos arquivos CSV

- TransformaÃ§Ãµes:

- ConversÃ£o de datas

- ConversÃ£o de valores monetÃ¡rios

- NormalizaÃ§Ã£o de colunas

- CÃ¡lculo de validade dos tÃ­tulos

## CriaÃ§Ã£o das tabelas:

- titulo

- movimentacao

- Carga automatizada no banco do projeto

- OrquestraÃ§Ã£o via Apache Airflow

## ğŸ‘¥ ColaboraÃ§Ã£o
- Projeto desenvolvido em parceria simulando ambiente real de mercado:

### ğŸ‘¨â€ğŸ’» Data Engineering
- Desenvolvimento do pipeline ETL

- Modelagem relacional

- ImplementaÃ§Ã£o da DAG

### âš™ï¸ DevOps
- ContainerizaÃ§Ã£o

- Infraestrutura Docker

- ConfiguraÃ§Ã£o do Airflow

### ğŸ“ˆ PossÃ­veis EvoluÃ§Ãµes
- Camada de API para exposiÃ§Ã£o dos dados

- Monitoramento e alertas

- Testes automatizados

- IntegraÃ§Ã£o com ferramentas de BI

- Data Quality Checks

### ğŸ’¡ Principais Aprendizados
- ConstruÃ§Ã£o de pipeline end-to-end

- IntegraÃ§Ã£o entre Airflow e PostgreSQL

- SeparaÃ§Ã£o entre banco de metadados e banco analÃ­tico

- ContainerizaÃ§Ã£o com Docker

- OrganizaÃ§Ã£o colaborativa em ambiente DataOps